{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bmv_dOpCKtlt"
      },
      "outputs": [],
      "source": [
        "# 📘 Proyecto de Regresión Profesional con Keras, GridSearchCV y SHAP\n",
        "\n",
        "# ============================\n",
        "# 1. Carga y preprocesamiento\n",
        "# ============================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Cargar dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# División y escalado\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ============================\n",
        "# 2. Definición del modelo\n",
        "# ============================\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers, Input, regularizers\n",
        "\n",
        "def build_model(n_hidden=2, n_neurons=64, learning_rate=0.01, dropout_rate=0.0, l2_reg=0.0):\n",
        "    input_layer = Input(shape=(X_train_scaled.shape[1],))\n",
        "    x = input_layer\n",
        "    for _ in range(n_hidden):\n",
        "        x = layers.Dense(n_neurons, activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
        "        if dropout_rate > 0:\n",
        "            x = layers.Dropout(dropout_rate)(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "\n",
        "    model = keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# ============================\n",
        "# 3. GridSearchCV\n",
        "# ============================\n",
        "\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "keras_reg = KerasRegressor(model=build_model, verbose=0)\n",
        "\n",
        "param_grid = {\n",
        "    \"model__n_hidden\": [1, 2],\n",
        "    \"model__n_neurons\": [32, 64],\n",
        "    \"model__learning_rate\": [0.001, 0.01],\n",
        "    \"model__dropout_rate\": [0.0, 0.2],\n",
        "    \"model__l2_reg\": [0.0, 0.01],\n",
        "    \"batch_size\": [16],\n",
        "    \"epochs\": [200]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(keras_reg, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nMejores hiperparámetros:\", grid_search.best_params_)\n",
        "\n",
        "# ============================\n",
        "# 4. Entrenamiento final y evaluación\n",
        "# ============================\n",
        "\n",
        "best_model = grid_search.best_estimator_.model_\n",
        "history = best_model.fit(X_train_scaled, y_train,\n",
        "                         validation_data=(X_test_scaled, y_test),\n",
        "                         epochs=300,\n",
        "                         callbacks=[keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)],\n",
        "                         verbose=0)\n",
        "\n",
        "def plot_learning_curves(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['mae'], label='Train MAE')\n",
        "    plt.plot(history.history['val_mae'], label='Val MAE')\n",
        "    plt.title('Error Absoluto Medio (MAE)')\n",
        "    plt.grid(); plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train MSE')\n",
        "    plt.plot(history.history['val_loss'], label='Val MSE')\n",
        "    plt.title('Error Cuadrático Medio (MSE)')\n",
        "    plt.grid(); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_curves(history)\n",
        "\n",
        "# ============================\n",
        "# 5. Evaluación en test\n",
        "# ============================\n",
        "\n",
        "predictions = best_model.predict(X_test_scaled)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(f\"\\nRMSE en test: {rmse:.2f}\")\n",
        "print(f\"R^2 en test: {r2:.2f}\")\n",
        "\n",
        "# ============================\n",
        "# 6. Interpretación con SHAP\n",
        "# ============================\n",
        "\n",
        "import shap\n",
        "\n",
        "explainer = shap.Explainer(best_model.predict, X_train_scaled[:100])\n",
        "shap_values = explainer(X_test_scaled[:10])\n",
        "\n",
        "# Gráfico local (1 muestra)\n",
        "shap.plots.waterfall(shap_values[0])\n",
        "\n",
        "# Importancia global\n",
        "shap.plots.beeswarm(shap_values)\n",
        "\n",
        "# ============================\n",
        "# 7. Posibles mejoras (nivel master)\n",
        "# ============================\n",
        "\n",
        "# - Probar modelos de ensamble (e.g., stacking con XGBoost)\n",
        "# - Validación cruzada más robusta (Repeated K-Fold)\n",
        "# - Automatización de hiperparámetros con Optuna o Keras Tuner\n",
        "# - Feature selection con SHAP + retrain\n",
        "# - EarlyStopping monitorizando 'val_mae' o custom callback\n",
        "# - Logging con TensorBoard para análisis detallado\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2VQQUzztKurA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g2kn2g_tKutG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}